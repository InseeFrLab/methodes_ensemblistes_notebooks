{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Premier exemple de classification: prédire le niveau de diplôme dans le recensement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le répertoire courant\n",
    "os.chdir('/home/onyxia/work/methodes_ensemblistes_notebooks')\n",
    "\n",
    "# Vérification\n",
    "print(\"Nouveau répertoire courant :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lire les fichiers Parquet\n",
    "def read_parquet_data(local_path, var_name, force_reload=False):\n",
    "    \"\"\"\n",
    "    Charge un fichier Parquet, avec possibilité de forcer le rechargement même si la variable est en mémoire.\n",
    "    \n",
    "    Args:\n",
    "        local_path (str): Chemin du fichier Parquet.\n",
    "        var_name (str): Nom de la variable à vérifier dans l'espace de noms global.\n",
    "        force_reload (bool): Forcer le rechargement des données même si elles sont déjà en mémoire.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Données chargées depuis le fichier Parquet.\n",
    "    \"\"\"\n",
    "    if var_name in globals() and not force_reload:\n",
    "        print(f\"Les données '{var_name}' sont déjà en mémoire. Utilisez force_reload=True pour les recharger.\")\n",
    "        return globals()[var_name]\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        try:\n",
    "            data = pq.read_table(local_path).to_pandas()\n",
    "            print(f\"Données Parquet chargées avec succès depuis : {local_path}\")\n",
    "            globals()[var_name] = data  # Stocker la variable dans l'espace global\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier Parquet : {e}\")\n",
    "    else:\n",
    "        print(f\"Le fichier n'existe pas : {local_path}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Fonction pour lire les fichiers CSV\n",
    "def read_csv_data(local_path, var_name, force_reload=False):\n",
    "    \"\"\"\n",
    "    Charge un fichier CSV, avec possibilité de forcer le rechargement même si la variable est en mémoire.\n",
    "    \n",
    "    Args:\n",
    "        local_path (str): Chemin du fichier CSV.\n",
    "        var_name (str): Nom de la variable à vérifier dans l'espace de noms global.\n",
    "        force_reload (bool): Forcer le rechargement des données même si elles sont déjà en mémoire.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Données chargées depuis le fichier CSV.\n",
    "    \"\"\"\n",
    "    if var_name in globals() and not force_reload:\n",
    "        print(f\"Les données '{var_name}' sont déjà en mémoire. Utilisez force_reload=True pour les recharger.\")\n",
    "        return globals()[var_name]\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        try:\n",
    "            data = pd.read_csv(local_path, sep=';')\n",
    "            print(f\"Données CSV chargées avec succès depuis : {local_path}\")\n",
    "            globals()[var_name] = data  # Stocker la variable dans l'espace global\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier CSV : {e}\")\n",
    "    else:\n",
    "        print(f\"Le fichier n'existe pas : {local_path}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données si elles ne sont pas déjà en mémoire\n",
    "data_census_individuals = read_parquet_data(\"data/data_census_individuals.parquet\", \"data_census_individuals\", force_reload=True)\n",
    "\n",
    "# Charger la documentation\n",
    "doc_census_individuals = read_csv_data(\"documentation/doc_census_individuals.csv\", \"doc_census_individuals\", force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Échantillonner les données (1/200)\n",
    "data_sample = data_census_individuals.sample(frac=1/200, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des variables liées à l'âge (autres que AGED) et des variables non pertinentes ou avec trop de modalités (pour commencer)\n",
    "columns_to_drop = [\n",
    "    'AGER20', 'AGEREV', 'AGEREVQ', 'ANAI', 'TRIRIS', 'IRIS', 'DNAI',\n",
    "    'DEPT', 'ARM', 'CANTVILLE', 'NUMMI', 'IPONDI'\n",
    "]\n",
    "data_clean = data_sample.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir la répartition des classes de diplôme\n",
    "print(data_clean['DIPL'].isna().sum())  # Vérification des valeurs manquantes\n",
    "print(data_clean['DIPL'].value_counts())  # Distribution des classes\n",
    "print(pd.crosstab(data_clean['DIPL'], data_clean['CS1'], normalize='columns') * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les données en features (X) et la variable cible (y : DIPL)\n",
    "X = data_clean.drop(columns=['DIPL'])\n",
    "y = data_clean['DIPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage \"one-hot\" des colonnes catégoriques\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Identifier les colonnes catégoriques\n",
    "categorical_cols = X.select_dtypes(include=['category']).columns\n",
    "\n",
    "# Appliquer le One-Hot Encoding sur les colonnes catégoriques\n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Convertir les colonnes encodées en DataFrame avec les noms des catégories\n",
    "X_encoded = pd.DataFrame(\n",
    "    X_encoded, \n",
    "    index=X.index, \n",
    "    columns=encoder.get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "# Sélectionner les colonnes numériques\n",
    "numeric_cols = X.select_dtypes(exclude=['category'])\n",
    "\n",
    "# Concaténer les colonnes numériques et encodées uniquement si les colonnes numériques ne sont pas vides\n",
    "if not numeric_cols.empty:\n",
    "    X = pd.concat([numeric_cols, X_encoded], axis=1)\n",
    "else:\n",
    "    X = X_encoded\n",
    "\n",
    "# Vérification finale\n",
    "print(f\"Dimensions finales après encodage : X={X.shape}\")\n",
    "print(\"Aperçu de X :\")\n",
    "print(X.head())\n",
    "\n",
    "# Vérifier les dimensions après division\n",
    "print(f\"Dimensions après division : X={X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions avant et après l'encodage\n",
    "print(f\"Dimensions avant encodage : {X.shape[0]} lignes, {categorical_cols.shape[0]} colonnes catégoriques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les poids des observations en fonction de l'inverse des fréquences dans y_train\n",
    "class_weights = {cls: 1 / count for cls, count in zip(*np.unique(y_train, return_counts=True))}\n",
    "class_weights_normalized = {cls: weight / sum(class_weights.values()) for cls, weight in class_weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Random Forest pour classification\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=class_weights_normalized,\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "import time\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Temps d'exécution du modèle Random Forest : {elapsed_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur les données de test\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer la performance avec une matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=rf_model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=rf_model.classes_)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion pour la prédiction du niveau de diplôme\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables (Mean Decrease in Impurity)\n",
    "importance_df = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rassembler l'importance des modalités d'une même variable\n",
    "def calculate_aggregated_importance(X, feature_importances):\n",
    "    \"\"\"\n",
    "    Agrège l'importance des variables catégorielles encodées en colonnes multiples.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Les données d'entraînement (features).\n",
    "        feature_importances (array): Les importances des variables calculées par le modèle.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Importance des variables agrégées par nom de variable.\n",
    "    \"\"\"\n",
    "    # Récupérer les noms des colonnes\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les importances agrégées\n",
    "    aggregated_importance = {}\n",
    "\n",
    "    for col in feature_names:\n",
    "        # Identifier les variables spécifiques pour regrouper correctement\n",
    "        if col.startswith('STAT_CONJ_'):\n",
    "            variable = 'STAT_CONJ'\n",
    "        elif col.startswith('STATR_'):\n",
    "            variable = 'STATR'\n",
    "        else:\n",
    "            # Utiliser le préfixe par défaut basé sur le premier segment avant \"_\"\n",
    "            variable = col.split('_')[0]\n",
    "        \n",
    "        # Additionner les importances pour chaque variable \"parent\"\n",
    "        if variable in aggregated_importance:\n",
    "            aggregated_importance[variable] += feature_importances[feature_names.get_loc(col)]\n",
    "        else:\n",
    "            aggregated_importance[variable] = feature_importances[feature_names.get_loc(col)]\n",
    "\n",
    "    # Convertir en DataFrame trié par importance\n",
    "    aggregated_df = pd.DataFrame({\n",
    "        'Variable': aggregated_importance.keys(),\n",
    "        'Importance': aggregated_importance.values()\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appeler la fonction pour regrouper les importances\n",
    "aggregated_importance_df = calculate_aggregated_importance(X, rf_model.feature_importances_)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(aggregated_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les libellés des variables depuis le dictionnaire\n",
    "doc_census_individuals_noms_variables = doc_census_individuals[['COD_VAR', 'LIB_VAR']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associer les libellés (descriptions) aux variables d'importance\n",
    "importance_df_with_labels = aggregated_importance_df.merge(\n",
    "    doc_census_individuals_noms_variables,\n",
    "    left_on='Variable',\n",
    "    right_on='COD_VAR',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour visualiser des n variables les plus importantes\n",
    "def plot_top_n_variables(importance_df, variable_col='Variable', importance_col='Importance', top_n=10):\n",
    "    \"\"\"\n",
    "    Visualise les n variables les plus importantes à partir d'un DataFrame d'importance.\n",
    "\n",
    "    Parameters:\n",
    "    - importance_df (pd.DataFrame): Un DataFrame contenant les colonnes spécifiées.\n",
    "    - variable_col (str): Le nom de la colonne contenant les noms des variables.\n",
    "    - importance_col (str): Le nom de la colonne contenant les valeurs d'importance.\n",
    "    - top_n (int): Le nombre de variables les plus importantes à afficher.\n",
    "    \"\"\"\n",
    "    # Vérifier si les colonnes existent\n",
    "    if variable_col not in importance_df.columns or importance_col not in importance_df.columns:\n",
    "        raise ValueError(f\"Les colonnes '{variable_col}' et/ou '{importance_col}' ne sont pas présentes dans le DataFrame.\")\n",
    "\n",
    "    # Trier par importance décroissante\n",
    "    top_variables = importance_df.sort_values(by=importance_col, ascending=False).head(top_n)\n",
    "\n",
    "    # Création du graphique\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(top_variables[variable_col], top_variables[importance_col], color='steelblue')\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Variable\")\n",
    "    plt.title(f\"Top {top_n} variables les plus importantes\")\n",
    "    plt.gca().invert_yaxis()  # Afficher les plus importantes en haut\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des variables les plus importantes\n",
    "plot_top_n_variables(\n",
    "    importance_df = importance_df_with_labels,\n",
    "    variable_col='LIB_VAR',\n",
    "    importance_col='Importance',\n",
    "    top_n=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
